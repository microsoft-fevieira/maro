# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import time
from os import getcwd
from typing import Callable, Dict, List, Union

from maro.rl.policy import AbsCorePolicy, LocalPolicyManager
from maro.utils import Logger

from .agent_wrapper import AgentWrapper
from .early_stopper import AbsEarlyStopper
from .env_wrapper import AbsEnvWrapper


class SimpleLearner:
    """Controller for single-threaded learning workflows.

    Args:
        env_wrapper (AbsEnvWrapper): Environment wrapper instance to interact with a set of agents and collect
            experiences for learning.
        agent_wrapper (AgentWrapper): Multi-policy wrapper that interacts with the ``env_wrapper`` directly.
        num_episodes (int): Number of training episodes. Each training episode may contain one or more
            collect-update cycles, depending on how the implementation of the roll-out manager.
        num_steps (int): Number of environment steps to roll out in each call to ``collect``. Defaults to -1, in which
            case the roll-out will be executed until the end of the environment.
        eval_schedule (Union[int, List[int]]): Evaluation schedule. If an integer is provided, the policies will
            will be evaluated every ``eval_schedule`` episodes. If a list is provided, the policies will be evaluated
            at the end of the training episodes given in the list. In any case, the policies will be evaluated
            at the end of the last training episode. Defaults to None, in which case the policies will only be
            evaluated after the last training episode.
        eval_env (AbsEnvWrapper): An ``AbsEnvWrapper`` instance for policy evaluation. If None, ``env`` will be used
            as the evaluation environment. Defaults to None.
        early_stopper (AbsEarlyStopper): Early stopper to stop the main training loop if certain conditions on the
            environment metrics are met following an evaluation episode. Default to None.
        update_trigger (Dict[str, int]): A dictionary of (policy_name, trigger), where "trigger" indicates the
            required number of new experiences to trigger a call to ``learn`` for each policy. Defaults to None,
            all triggers will be set to 1.
        warmup (Dict[str, int]): A dictionary of (policy_name, warmup_size), where "warmup_size" indicates the
            minimum number of experiences in the experience memory required to trigger a call to ``learn`` for
            each policy. Defaults to None, in which case all warm-up sizes will be set to 1.
        post_collect (Callable): Custom function to process whatever information is collected by each
            environment wrapper (local or remote) at the end of ``collect`` calls. The function signature should
            be (trackers,) -> None, where tracker is a list of environment wrappers' ``tracker`` members. Defaults
            to None.
        post_evaluate (Callable): Custom function to process whatever information is collected by each
            environment wrapper (local or remote) at the end of ``evaluate`` calls. The function signature should
            be (trackers,) -> None, where tracker is a list of environment wrappers' ``tracker`` members. Defaults
            to None.
        post_update (Callable): Custom function to process whatever information is collected by each
            trainer (local or remote) at the end of ``update`` calls. The function signature should be (trackers,
            ) -> None, where tracker is a list of environment wrappers' ``tracker`` members. Defaults to
            None.
        log_dir (str): Directory to store logs in. A ``Logger`` with tag "LOCAL_ROLLOUT_MANAGER" will be created at init
            time and this directory will be used to save the log files generated by it. Defaults to the current working
            directory.
    """

    def __init__(
        self,
        env_wrapper: AbsEnvWrapper,
        agent_wrapper: AgentWrapper,
        num_episodes: int,
        num_steps: int = -1,
        eval_schedule: Union[int, List[int]] = None,
        eval_env: AbsEnvWrapper = None,
        early_stopper: AbsEarlyStopper = None,
        update_trigger: Dict[str, int] = None,
        warmup: Dict[str, int] = None,
        post_collect: Callable = None,
        post_evaluate: Callable = None,
        post_update: Callable = None,
        log_dir: str = getcwd(),
    ):
        if num_steps == 0 or num_steps < -1:
            raise ValueError("num_steps must be a positive integer or -1")

        self._logger = Logger("LOCAL_LEARNER", dump_folder=log_dir)
        self.env = env_wrapper
        self.eval_env = eval_env if eval_env else self.env
        self.agent = agent_wrapper

        # Create a policy manager to manage all trainable policies from the agent wrapper
        self.policy_manager = LocalPolicyManager(
            {name: policy for name, policy in self.agent.policy_dict.items() if isinstance(policy, AbsCorePolicy)},
            update_trigger=update_trigger,
            warmup=warmup,
            post_update=post_update
        )

        self.num_episodes = num_episodes
        self._num_steps = num_steps if num_steps > 0 else float("inf")

        # evaluation schedule
        if eval_schedule is None:
            self._eval_schedule = []
        elif isinstance(eval_schedule, int):
            num_eval_schedule = num_episodes // eval_schedule
            self._eval_schedule = [eval_schedule * i for i in range(1, num_eval_schedule + 1)]
        else:
            self._eval_schedule = eval_schedule
            self._eval_schedule.sort()

        # always evaluate after the last episode
        if not self._eval_schedule or num_episodes != self._eval_schedule[-1]:
            self._eval_schedule.append(num_episodes)

        self._logger.info(f"Policy will be evaluated at the end of episodes {self._eval_schedule}")
        self._eval_point_index = 0

        self.early_stopper = early_stopper
        self._post_collect = post_collect
        self._post_evaluate = post_evaluate
        self._post_update = post_update

    def run(self):
        """Entry point for executing a learning workflow."""
        for ep in range(1, self.num_episodes + 1):
            self._collect_and_update(ep)
            if ep == self._eval_schedule[self._eval_point_index]:
                self._eval_point_index += 1
                self._evaluate(self._eval_point_index)
                # early stopping check
                if self.early_stopper:
                    self.early_stopper.push(self.eval_env.summary)
                    if self.early_stopper.stop():
                        return

    def _collect_and_update(self, ep: int):
        """Collect simulation data for training."""
        t0 = time.time()
        num_experiences_collected = 0

        self.agent.explore()
        self.env.reset()
        self.env.start()  # get initial state
        segment = 0
        while self.env.state:
            segment += 1
            self._collect(ep, segment)
            exp_by_policy = self.agent.get_batch(self.env)
            self.policy_manager.update(exp_by_policy)
            num_experiences_collected += sum(exp.size for exp in exp_by_policy.values())
        # update the exploration parameters if an episode is finished
        self.agent.exploration_step()

        self._logger.info(
            f"ep {ep} summary - "
            f"running time: {time.time() - t0} "
            f"env steps: {self.env.step_index} "
            f"experiences collected: {num_experiences_collected}"
        )

    def _collect(self, ep, segment):
        start_step_index = self.env.step_index + 1
        steps_to_go = self._num_steps
        while self.env.state and steps_to_go:
            self.env.step(self.agent.choose_action(self.env.state))
            steps_to_go -= 1

        self._logger.info(
            f"Roll-out finished for ep {ep}, segment {segment}"
            f"(steps {start_step_index} - {self.env.step_index})"
        )

        if self._post_collect:
            self._post_collect([self.env.tracker], ep, segment)

    def _evaluate(self, ep: int):
        """Policy evaluation."""
        self._logger.info("Evaluating...")
        self.agent.exploit()
        self.eval_env.reset()
        self.eval_env.start()  # get initial state
        while self.eval_env.state:
            self.eval_env.step(self.agent.choose_action(self.eval_env.state))

        if self._post_evaluate:
            self._post_evaluate([self.env.tracker], ep)
